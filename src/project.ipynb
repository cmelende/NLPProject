{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Cory Melendez\n",
    "###### Natural Language Processing Project\n",
    "###### https://github.com/cmelende/NLPProject.git\n",
    "###### 12/11/20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### 1. Import libraries, load dataset print shape of data, data description"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-13e7d8e21e97>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mre\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software projects\\utpgpaiml\\nlpproject\\venv\\lib\\site-packages\\pandas\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_print_versions\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mshow_versions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 142\u001B[1;33m from pandas.io.api import (\n\u001B[0m\u001B[0;32m    143\u001B[0m     \u001B[1;31m# excel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m     \u001B[0mExcelFile\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software projects\\utpgpaiml\\nlpproject\\venv\\lib\\site-packages\\pandas\\io\\api.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclipboards\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mread_clipboard\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExcelWriter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mread_excel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeather_format\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mread_feather\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgbq\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mread_gbq\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software projects\\utpgpaiml\\nlpproject\\venv\\lib\\site-packages\\pandas\\io\\excel\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_base\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExcelWriter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mread_excel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_odswriter\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_ODSWriter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_openpyxl\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_OpenpyxlWriter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_util\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mregister_writer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexcel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_xlsxwriter\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_XlsxWriter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software projects\\utpgpaiml\\nlpproject\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_config\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_libs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparsers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSTR_NA_VALUES\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEmptyDataError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_decorators\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAppender\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdeprecate_nonkeyword_arguments\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\Python38\\lib\\importlib\\_bootstrap.py\u001B[0m in \u001B[0;36mparent\u001B[1;34m(self)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_source = './data/Tweets.csv'\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "df = pd.read_csv(data_source)\n",
    "print(\"Shape of Data is: \", df.shape)\n",
    "print(\"Description of data is: /n\", df.describe())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tweet_col = 'tweet_id'\n",
    "airline_sentiment = 'airline_sentiment'\n",
    "airline_sentiment_confidence = 'airline_sentiment_confidence'\n",
    "negative_reason = 'negativereason'\n",
    "negative_reason_confidence = 'negativereason_confidence'\n",
    "airline = 'airline'\n",
    "airline_sentiment_gold = 'airline_sentiment_gold'\n",
    "name = 'name'\n",
    "negative_reason_gold = 'negativereason_gold'\n",
    "retweet_count = 'retweet_count'\n",
    "text = 'text'\n",
    "tweet_coord = 'tweet_coord'\n",
    "tweet_created = 'tweet_created'\n",
    "tweet_location = 'tweet_location'\n",
    "user_timezone = 'user_timezone'\n",
    "all_cols = [tweet_col, airline_sentiment, airline_sentiment_confidence,\n",
    "            negative_reason, negative_reason_confidence, airline,\n",
    "            airline_sentiment_gold, name, negative_reason_gold,\n",
    "            retweet_count, text, tweet_coord, tweet_created,\n",
    "            tweet_location, user_timezone]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2. Understand the data columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### a. Drop all other columns except 'text' and 'airline_sentiment'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_columns(df, keep_columns, all_columns):\n",
    "    copy = pd.DataFrame()\n",
    "    for col in all_columns:\n",
    "        if col in keep_columns:\n",
    "            copy[col] = df[col]\n",
    "\n",
    "    return copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### b. Check the shape of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trimmed_df = remove_columns(df, [text, airline_sentiment], all_cols)\n",
    "print(\"shape: \", trimmed_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### c. Print the first 5 rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"first 5 rows\")\n",
    "print(trimmed_df.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. Text Pre-processing: Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### a. Html tag removal\n",
    "###### c. Remove the numbers\n",
    "###### d. remove special characters and punctuations\n",
    "###### e. conversion to lowercase\n",
    "###### g. join the words in the list to convert back to text string in the dataframe (So that each row contains the data in text format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")    # Removing HTML tags\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_special_characters_numbers(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_special_characters_numbers(text)\n",
    "    text = to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "cleaned_df = trimmed_df.copy()\n",
    "cleaned_df[text] = trimmed_df[text].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### b. Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer=ToktokTokenizer()\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    token_array = tokenize(text)\n",
    "    words = [w for w in token_array if not w in english_stopwords]\n",
    "    reassembled_string = reassemble_token_array(words)\n",
    "    return reassembled_string\n",
    "\n",
    "def reassemble_token_array(token_array):\n",
    "    space = ' '\n",
    "    reassembled_string = space.join(token_array)\n",
    "    return reassembled_string\n",
    "\n",
    "cleaned_df[text] = cleaned_df[text].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### f. lemmatatize or stemming\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "cleaned_df[text] = cleaned_df[text].apply(lemmatize_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### h. print first 5 rows of data after pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}